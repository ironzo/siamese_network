{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44cd7e7a-ee9e-426f-87d8-cfecdce2412b",
   "metadata": {},
   "source": [
    "In this notebook we will define and train the **Siamese Networks** which is a powerfull tool for defining 2 similar objects. In this example we will take text as an object, but the Siamese Network can also detect similar images (say signature checking, face recognition etc.). <br>\n",
    "The idea is create two sub-networks (sister-networks) which share identical parameters. This means that you **only** need to train one set of weights. The output of each sub-network is a vector. You can then run the output through a cosine similarity function to get the similarity score. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6552c45-03ff-4d9e-9046-234c16b2750c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dedc4df0-95f6-45cb-87a1-170391d6dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e2628d-ffe4-4c76-a7e5-c9abb33b9096",
   "metadata": {},
   "source": [
    "I used \"Quora question answer\" dataset to create a model which could predict if 2 questions are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74ebec35-46cc-4c25-b29e-f4b76a3bbbfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of question pairs:  404351\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"questions.csv\")\n",
    "N = len(data)\n",
    "print('Number of question pairs: ', N)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f45638-afad-4b01-91ac-7e72df65a23d",
   "metadata": {},
   "source": [
    "We have pairs of questions (question1 and question2) and in the column is_duplicate 0 - no, 1 - yes. Also each question has its id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42cb9db-4922-45af-a5b7-436d4a9f9440",
   "metadata": {},
   "source": [
    "### Train-Val-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66585b5e-476a-410f-8345-d86bcae657a9",
   "metadata": {},
   "source": [
    "Let's start with splitting the dataset into train and test. Around 74% - train and all the rest - test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b862388d-fa2e-40aa-863a-d897685405ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 300000 Test set: 10240\n"
     ]
    }
   ],
   "source": [
    "N_train = 300000\n",
    "N_test = 10240\n",
    "data_train = data[:N_train]\n",
    "data_test = data[N_train:N_train + N_test]\n",
    "print(\"Train set:\", len(data_train), \"Test set:\", len(data_test))\n",
    "del (data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528c05e-964d-4506-8589-7683cd7e148b",
   "metadata": {},
   "source": [
    "To train the Siamese Network we will give to her pairs of duplicated questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa1ccb09-0d16-4637-b550-0308491942a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_index = data_train['is_duplicate'] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a7f6790-f49d-46aa-8f74-8d58b1c44430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate questions:  111486\n",
      "Indexes of first ten duplicate questions: [5, 7, 11, 12, 13, 15, 16, 18, 20, 29]\n"
     ]
    }
   ],
   "source": [
    "td_index = [i for i, x in enumerate(td_index) if x]\n",
    "print('Number of duplicate questions: ', len(td_index))\n",
    "print('Indexes of first ten duplicate questions:', td_index[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc288f88-9565-4487-898f-d546cfbe30d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\n",
      "I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\n",
      "is_duplicate:  1\n"
     ]
    }
   ],
   "source": [
    "print(data_train['question1'][5])\n",
    "print(data_train['question2'][5])\n",
    "print('is_duplicate: ', data_train['is_duplicate'][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e640699-73ff-40dc-acae-1292f065f08d",
   "metadata": {},
   "source": [
    "Keeping only the rows in the original training set that correspond to the rows where td_index is True --> Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1e9f435-b1b7-4fda-bb19-699a47ec027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_train = np.array(data_train['question1'][td_index])\n",
    "Q2_train = np.array(data_train['question2'][td_index])\n",
    "\n",
    "Q1_test = np.array(data_test['question1'])\n",
    "Q2_test = np.array(data_test['question2'])\n",
    "y_test  = np.array(data_test['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3a2cd7d-69c0-484b-87d9-72cde0c6f88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING QUESTIONS:\n",
      "\n",
      "Question 1:  Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\n",
      "Question 2:  I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me? \n",
      "\n",
      "Question 1:  What would a Trump presidency mean for current international master’s students on an F1 visa?\n",
      "Question 2:  How will a Trump presidency affect the students presently in US or planning to study in US? \n",
      "\n",
      "TESTING QUESTIONS:\n",
      "\n",
      "Question 1:  How do I prepare for interviews for cse?\n",
      "Question 2:  What is the best way to prepare for cse? \n",
      "\n",
      "is_duplicate = 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('TRAINING QUESTIONS:\\n')\n",
    "print('Question 1: ', Q1_train[0])\n",
    "print('Question 2: ', Q2_train[0], '\\n')\n",
    "print('Question 1: ', Q1_train[5])\n",
    "print('Question 2: ', Q2_train[5], '\\n')\n",
    "\n",
    "print('TESTING QUESTIONS:\\n')\n",
    "print('Question 1: ', Q1_test[0])\n",
    "print('Question 2: ', Q2_test[0], '\\n')\n",
    "print('is_duplicate =', y_test[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addae78a-bf8f-442c-9d61-b7f1f5c6b49a",
   "metadata": {},
   "source": [
    "Splitting training set into training/validation sets 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21d27c70-7652-4c8f-a405-493ca241bef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate questions:  111486\n",
      "The length of the training set is:   89188\n",
      "The length of the validation set is:  22298\n"
     ]
    }
   ],
   "source": [
    "cut_off = int(len(Q1_train) * 0.8)\n",
    "train_Q1, train_Q2 = Q1_train[:cut_off], Q2_train[:cut_off]\n",
    "val_Q1, val_Q2 = Q1_train[cut_off:], Q2_train[cut_off:]\n",
    "print('Number of duplicate questions: ', len(Q1_train))\n",
    "print(\"The length of the training set is:  \", len(train_Q1))\n",
    "print(\"The length of the validation set is: \", len(val_Q1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205afe26-85d7-492d-8e01-59e2d4056d17",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb4df2-8a7e-4492-9f94-2ae4cfa66ac2",
   "metadata": {},
   "source": [
    "Now once we need to take care of the encoding (transfering text into numbers). \n",
    "We can start with learning (using .adapt() )all words from training dataset and we will use these words (word dictionary) to encode each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8518a4f6-8ae1-4a4f-b027-a3a556064fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "text_vectorization = tf.keras.layers.TextVectorization(output_mode='int',split='whitespace', standardize='strip_punctuation')\n",
    "text_vectorization.adapt(np.concatenate((train_Q1,train_Q2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "befc8fa3-9760-4f5b-9c5b-42b15729014b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32819\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocabulary size: {text_vectorization.vocabulary_size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43ac4ed4-c09f-4702-a96e-5fd591e1b918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first question in the train set:\n",
      "\n",
      "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me? \n",
      "\n",
      "encoded version:\n",
      "tf.Tensor(\n",
      "[ 6123     6   178    10  9079  2220 32055   788    13  6047 25433    30\n",
      "    28   463    45    98], shape=(16,), dtype=int64) \n",
      "\n",
      "first question in the test set:\n",
      "\n",
      "How do I prepare for interviews for cse? \n",
      "\n",
      "encoded version:\n",
      "tf.Tensor([    4     8     6   157    17  1909    17 11616], shape=(8,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print('first question in the train set:\\n')\n",
    "print(Q1_train[0], '\\n') \n",
    "print('encoded version:')\n",
    "print(text_vectorization(Q1_train[0]),'\\n')\n",
    "\n",
    "print('first question in the test set:\\n')\n",
    "print(Q1_test[0], '\\n')\n",
    "print('encoded version:')\n",
    "print(text_vectorization(Q1_test[0]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa25a1de-9127-47b8-849d-8c8c2a8da96f",
   "metadata": {},
   "source": [
    "### Define the Siamese Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc61f542-23cd-4cb5-9336-ef30ebc5a7bf",
   "metadata": {},
   "source": [
    "**The model's architecture:** <br>\n",
    "Input 1 and 2 - 2 questions <br>\n",
    "Embedding <br>\n",
    "LSTM Layer - our NN <br>\n",
    "Output - vector 1 and vector 2 (we will calculate cosine similarity for them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "35f7eea5-e4fc-456e-b301-d5f1d60030b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Siamese(text_vectorizer, vocab_size, d_feature=128):\n",
    "    branch = tf.keras.Sequential([\n",
    "        text_vectorizer,\n",
    "        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=d_feature, name=\"embedding\"),\n",
    "        tf.keras.layers.LSTM(units=d_feature, return_sequences=True, name=\"LSTM\"),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(name='mean'),\n",
    "        tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1), name='out')\n",
    "    ], name='sequential')\n",
    "\n",
    "    input1 = tf.keras.layers.Input(shape=(1,), dtype=tf.string, name='input_1')\n",
    "    input2 = tf.keras.layers.Input(shape=(1,), dtype=tf.string, name='input_2')\n",
    "\n",
    "    processed1 = branch(input1)\n",
    "    processed2 = branch(input2)\n",
    "\n",
    "    conc = tf.keras.layers.Concatenate(axis=1, name='conc_1_2')([processed1, processed2])\n",
    "\n",
    "    return tf.keras.models.Model(inputs=[input1, input2], outputs=conc, name=\"SiameseModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c207aea1-cf74-4c66-872b-b48e6c7baecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"SiameseModel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"SiameseModel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,332,416</span> │ input_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │ input_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conc_1_2            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ sequential[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │  \u001b[38;5;34m4,332,416\u001b[0m │ input_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │ input_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conc_1_2            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ sequential[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ sequential[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,332,416</span> (16.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,332,416\u001b[0m (16.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,332,416</span> (16.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,332,416\u001b[0m (16.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,200,832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ LSTM (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mean (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │     \u001b[38;5;34m4,200,832\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ LSTM (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mean (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ out (\u001b[38;5;33mLambda\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,332,416</span> (16.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,332,416\u001b[0m (16.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,332,416</span> (16.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,332,416\u001b[0m (16.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "odel = Siamese(text_vectorization, vocab_size=vocab_size)\n",
    "model.build(input_shape=None)\n",
    "model.summary()\n",
    "model.get_layer(name='sequential').summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6646ab53-5300-445d-999b-4620a5764f1d",
   "metadata": {},
   "source": [
    "### Triplet Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e48646-3625-4d82-b173-6ffc501c5bc5",
   "metadata": {},
   "source": [
    "One of loss functions which can be used with Siamese Model is Triplet Loss Function. The idea is to take the Anchor (baseline) vector, Positive vector (should be the most similar with Anchor) and Negative vector (should be different from Anchor). **We need to minimize the distance between Anchor and Positive vectors and maximize it between Ancor and Negative vectors.** <br>\n",
    "$$\\mathcal{L}(A, P, N)=\\max \\left(\\|\\mathrm{f}(A)-\\mathrm{f}(P)\\|^{2}-\\|\\mathrm{f}(A)-\\mathrm{f}(N)\\|^{2}+\\alpha, 0\\right),$$ <br> where alpha is a margin (hyperparameter used to enforce a margin between similar and dissimilar examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dee1f9-0208-4f66-9235-89df1588d2bd",
   "metadata": {},
   "source": [
    "To implement Triplet Loss Function we can use **Negative Hard Mining**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "31c28ffd-11cb-4237-bdd1-bc9d12ff6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TripletLossFn(v1, v2, margin=0.25):\n",
    "    # Input - we take set of 2 embeddings (v1 and v2, where one of the is anchor and another is pos or neg.\n",
    "    # Input - also we will define margin (which is alpha).\n",
    "\n",
    "    # Start with calculating cosine similarities between each vector.\n",
    "    scores = tf.linalg.matmul(v2, v1, transpose_b=True)\n",
    "    batch_size = tf.cast(tf.shape(v1)[0], scores.dtype) \n",
    "\n",
    "    #The diagonal elements of the scores matrix correspond to the similarity scores of positive pairs, \n",
    "    # where the corresponding embeddings in v1 and v2 are from the same class.\n",
    "    positive = tf.linalg.diag_part(scores)\n",
    "\n",
    "    # Now subtract all positives to leave only negatives and calculate them\n",
    "    negative_zero_on_duplicate = scores - tf.linalg.diag(positive)\n",
    "\n",
    "    # The mean of the negative scores is computed for each anchor, excluding the positive score\n",
    "    mean_negative = tf.math.reduce_sum(negative_zero_on_duplicate, axis=1) / (batch_size - 1)\n",
    "\n",
    "    # Now find \"the hardest negative\" - most similar non-positive pair \n",
    "    mask_exclude_positives = tf.cast((tf.eye(batch_size) == 1)|(negative_zero_on_duplicate > tf.expand_dims(positive, 1)),\n",
    "                                    scores.dtype) # creates a mask to ignore diagonal elements (positives) and to filter out non-hard negatives\n",
    "    negative_without_positive = negative_zero_on_duplicate - mask_exclude_positives * 2.0 # removes the influence of positive pairs and non-hard negatives\n",
    "    closest_negative = tf.math.reduce_max(negative_without_positive, axis=1) # identifies the hardest negative for each anchor, i.e., the negative sample most similar to the anchor\n",
    "    \n",
    "    # This penalizes based on the hardest negative, ensuring that the hardest negative is at least margin away from the positive.\n",
    "    triplet_loss1 = tf.maximum(0.0, margin - positive + closest_negative)\n",
    "\n",
    "    # This penalizes based on the mean negative score, ensuring the average negative is at least margin away from the positive.\n",
    "    triplet_loss2 = tf.maximum(0.0, margin - positive + mean_negative)\n",
    "\n",
    "    # The final triplet loss is the sum of the two computed losses.\n",
    "    triplet_loss = tf.math.reduce_sum(triplet_loss1 + triplet_loss2)\n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cf1c773f-6e0b-4313-ad31-fc2226d1a6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss: 0.703507682515891\n"
     ]
    }
   ],
   "source": [
    "v1 = np.array([[0.26726124, 0.53452248, 0.80178373],[0.5178918 , 0.57543534, 0.63297887]])\n",
    "v2 = np.array([[ 0.26726124,  0.53452248,  0.80178373],[-0.5178918 , -0.57543534, -0.63297887]])\n",
    "print(\"Triplet Loss:\", TripletLossFn(v1,v2).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ab8f6114-d153-4948-bed6-f77dec26e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TripletLoss(labels, out, margin=0.25):\n",
    "    _, out_size = out.shape\n",
    "    v1 = out[:,:int(out_size/2)]\n",
    "    v2 = out[:,int(out_size/2):]\n",
    "    return TripletLossFn(v1, v2, margin=margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e250b92-dd38-4c88-90d4-0bf561e1bed0",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47253056-36fc-44d7-9984-802c703386a1",
   "metadata": {},
   "source": [
    "Now we will train the model on train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4194102-20e1-4e00-bc08-6ba75459a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(((train_Q1, train_Q2), tf.constant([1] * len(train_Q1))))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(((val_Q1, val_Q2), tf.constant([1] * len(val_Q1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ed966114-a7eb-41ab-b3d5-118784a4162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(Siamese, TripletLoss, text_vectorizer, train_dataset, val_dataset, d_feature=128, lr=0.01, train_steps=5):\n",
    "    model = Siamese(text_vectorizer, vocab_size=text_vectorizer.vocabulary_size(), d_feature=d_feature)\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=TripletLoss, optimizer=adam)\n",
    "    model.fit(train_dataset, epochs=train_steps, validation_data=val_dataset)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3278647a-5ada-4a8c-88ea-3f7fe0db05e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = 2\n",
    "batch_size = 256\n",
    "train_generator = train_dataset.shuffle(len(train_Q1), seed=7, reshuffle_each_iteration=True).batch(batch_size=batch_size)\n",
    "val_generator = val_dataset.shuffle(len(val_Q1), seed=7, reshuffle_each_iteration=True).batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ffef7bd5-9eee-4238-a5f7-aad60e26baa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 92ms/step - loss: 59.2880 - val_loss: 15.5867\n",
      "Epoch 2/2\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 97ms/step - loss: 11.5755 - val_loss: 11.1240\n"
     ]
    }
   ],
   "source": [
    "model = train_model(Siamese, TripletLoss, text_vectorization, train_generator, val_generator, train_steps=train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766974f5-4438-4538-9494-7cccb88244b5",
   "metadata": {},
   "source": [
    "### Evaluating Siamese Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c88e54-53ba-49eb-b58e-3bd5c0919e83",
   "metadata": {},
   "source": [
    "We will evaluate the accuracy of the model over the defined threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5c8e2018-d1c0-48ec-b9b4-449d71b92a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the accuracy of the model\n",
    "def classify(test_Q1, test_Q2, y_test, threshold, model, batch_size=64, verbose=True):\n",
    "    y_pred = []\n",
    "    test_gen = tf.data.Dataset.from_tensor_slices(((test_Q1, test_Q2),None)).batch(batch_size=batch_size)\n",
    "    \n",
    "    pred = model.predict(test_gen)\n",
    "    _, n_feat = pred.shape\n",
    "    v1 = pred[:,:int(n_feat/2)]\n",
    "    v2 = pred[:,int(n_feat/2):]\n",
    "    \n",
    "    d  = tf.math.reduce_sum(tf.multiply(v1, v2), axis = 1)\n",
    "\n",
    "    y_pred = tf.cast(d > threshold , tf.float64)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_test), tf.float32))\n",
    "    cm = tf.math.confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "    return accuracy, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a3778f28-8095-435c-b03c-1e0a458de372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step\n",
      "Accuracy 0.73525393\n",
      "Confusion matrix:\n",
      "[[4442 1940]\n",
      " [ 771 3087]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating...\n",
    "accuracy, cm = classify(Q1_test,Q2_test, y_test, 0.7, model,  batch_size = 512) \n",
    "print(\"Accuracy\", accuracy.numpy())\n",
    "print(f\"Confusion matrix:\\n{cm.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "60ea4d5f-a1b0-4a78-abc2-61a8bd41cdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.6960200564086493\n",
      "recall = 0.8521005179359294\n"
     ]
    }
   ],
   "source": [
    "tp = 4442\n",
    "fp = 1940\n",
    "fn = 771\n",
    "tn = 3087\n",
    "\n",
    "precision = (tp/(tp+fp))\n",
    "print(f'precision = {precision}')\n",
    "\n",
    "recall = (tp/(tp+fn))\n",
    "print(f'recall = {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3b03f6-3cfe-47cd-8442-44870ed0d5df",
   "metadata": {},
   "source": [
    "**Conclusions:** <br>\n",
    "**Accuracy - about 73.5%** of the predictions made by the Siamese Network are correct. While this is a reasonable level of accuracy, it indicates that there is still room for improvement. <br>\n",
    "**False Positive Rate (FPR):** <br>\n",
    "The number of false positives (1940) is relatively high compared to true positives (4442). This indicates that the model is prone to **incorrectly predicting dissimilar pairs as similar.** <br>\n",
    "**False Negative Rate (FNR):** <br>\n",
    "The false negatives (771) are lower than the false positives but still significant. The model **misses some pairs that are actually similar**. <br>\n",
    "**Precision and Recall:** <br>\n",
    "The model is more focused on recalling similar pairs (fewer misses) than on precision (more false alarms).<br>\n",
    "**Balance Between Classes:** <br>\n",
    "The confusion matrix indicates a significant number of both positive and negative examples, meaning the dataset is relatively balanced. However, the model's performance on predicting negatives (similar) seems weaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10739297-6fd2-42a6-b679-c899703763f5",
   "metadata": {},
   "source": [
    "### Predict Function to test out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0afd83f6-6650-454a-b60a-9fa47f808f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLAYGROUND\n",
    "def predict(question1, question2, threshold, model, verbose=False):\n",
    "    q1 = np.array([question1])\n",
    "    q2 = np.array([question2])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((q1, q2),None)).batch(batch_size=1)\n",
    "    \n",
    "    pred = model.predict(dataset)\n",
    "    _, n_feat = pred.shape\n",
    "    v1 = pred[:,:int(n_feat/2)]\n",
    "    v2 = pred[:,int(n_feat/2):]\n",
    "\n",
    "    \n",
    "    d  = tf.math.reduce_sum(tf.multiply(v1, v2), axis = 1)\n",
    "    res = d > threshold\n",
    "\n",
    "    message = f'Cosine Similarity = {d.numpy()}, Result: {res.numpy()}'\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Q1  = \", question1, \"\\nQ2  = \", question2)\n",
    "        print(\"d   = \", d.numpy())\n",
    "        print(\"res = \", res.numpy())\n",
    "        \n",
    "    return message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8ce9203a-2245-4c13-a09c-91807039e23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Prediction result: Cosine Similarity = [0.88742036], Result: [ True]\n"
     ]
    }
   ],
   "source": [
    "question1 = \"When will I see you?\"\n",
    "question2 = \"When can I see you again?\"\n",
    "\n",
    "result = predict(question1, question2, 0.65, model)\n",
    "print(\"Prediction result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "632162ad-c08d-4e09-a689-b584200f93b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Prediction result: Cosine Similarity = [0.69174474], Result: [ True]\n"
     ]
    }
   ],
   "source": [
    "question1 = \"What city is the capital of USA?\"\n",
    "question2 = \"What is the US capital?\"\n",
    "\n",
    "result = predict(question1, question2, 0.65, model)\n",
    "print(\"Prediction result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "37e72fc1-31c3-4457-b014-da2fbac4ef5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Prediction result: Cosine Similarity = [0.67481375], Result: [ True]\n"
     ]
    }
   ],
   "source": [
    "question1 = \"Do you enjoy reading books in the free time?\"\n",
    "question2 = \"Do you like to read novels when you have spare time?\"\n",
    "\n",
    "result = predict(question1, question2, 0.65, model)\n",
    "print(\"Prediction result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0116d0c5-1e67-41c6-ac59-02fa01213c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Prediction result: Cosine Similarity = [0.3179947], Result: [False]\n"
     ]
    }
   ],
   "source": [
    "question1 = \"Where did you find your cat?\"\n",
    "question2 = \"When did your cat get back home?\"\n",
    "\n",
    "result = predict(question1, question2, 0.65, model)\n",
    "print(\"Prediction result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "531b153e-4381-469c-8b1c-1001f3c18a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Prediction result: Cosine Similarity = [0.6256363], Result: [False]\n"
     ]
    }
   ],
   "source": [
    "question1 = \"Do they enjoy eating the dessert?\"\n",
    "question2 = \"Do they like hiking in the desert?\"\n",
    "result = predict(question1, question2, 0.65, model)\n",
    "print(\"Prediction result:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
